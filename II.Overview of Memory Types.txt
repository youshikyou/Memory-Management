2.3 Types of Computer Memory
    In a course on memory management we obviously need to take a look at the available memory types in computer systems. Below you will find a small list of some common memory types that you will surely have heard of:

    RAM / ROM
    Cache (L1, L2)
    Registers
    Virtual Memory
    Hard Disks, USB drives
    When the CPU of a computer needs to access memory, it wants to do this with minimal latency. 
    Also, as large amounts of information need to be processed, the available memory should be sufficiently large with regard to the tasks 
    we want to accomplish.

    Regrettably though, low latency and large memory are not compatible with each other (at least not at a reasonable price). 
    In practice, the decision for low latency usually results in a reduction of the available storage capacity (and vice versa). 
    This is the reason why a computer has multiple memory types that are arranged hierarchically. 
    The 2.3.1figure pyramid illustrates the principle:
    As you can see, the CPU and its ultra-fast (but small) registers used for short-term data storage reside at the top of the pyramid. 
    Below are Cache and RAM, which belong to the category of temporary memory which quickly looses its content once power is cut off. 
    Finally, there are permanent storage devices such as the ROM, hard drives as well as removable drives such as USB sticks.

    Let us take a look at a typical computer usage scenario to see how the different types of memory are used:

    After switching on the computer, it loads data from its read-only memory (ROM) and performs a power-on self-test (POST) 
    to ensure that all major components are working properly. 
    Additionally, the computer memory controller checks all of the memory addresses with a simple read/write operation 
    to ensure that memory is functioning correctly.

    After performing the self-test, the computer loads the basic input/output system (BIOS) from ROM. 
    The major task of the BIOS is to make the computer functional by providing basic information about such things as storage devices,
    boot sequence, security or auto device recognition capability.

    The process of activating a more complex system on a simple system is called "bootstrapping": 
    It is a solution for the chicken-egg-problem of starting a software-driven system by itself using software. 
    During bootstrapping, the computer loads the operating system (OS) from the hard drive into random access memory (RAM). 
    RAM is considered "random access" because any memory cell can be accessed directly 
    by intersecting the respective row and column in the matrix-like memory layout. 
    For performance reasons, many parts of the OS are kept in RAM as long as the computer is powered on.

    When an application is started, it is loaded into RAM. However, several application components are only loaded into RAM on demand 
    to preserve memory. Files that are opened during runtime are also loaded into RAM. 
    When a file is saved, it is written to the specified storage device. After closing the application, it is deleted from RAM.

    This simple usage scenario shows the central importance of the RAM. Every time data is loaded or a file is opened, 
    it is placed into this temporary storage area - but what about the other memory types above the RAM layer in the pyramid?

    To maximize CPU performance, fast access to large amounts of data is critical. If the CPU cannot get the data it needs, 
    it stops and waits for data availability. Thus, when designing new memory chips, engineers must adapt to the speed of the available CPUs. 
    The problem they are facing is that memory which is able to keep up with modern CPUs running at several GHz is extremely expensive. 
    To combat this, computer designers have created the memory tier system which has already been shown in the pyramid diagram above. 
    The solution is to use expensive memory in small quantities and then back it up using larger quantities of less expensive memory.

    The cheapest form of memory available today is the hard disk. It provides large quantities of inexpensive and permanent storage. 
    The problem of a hard disk is its comparatively low speed - even though access times with modern solid state disks (SSD) 
    have decreased significantly compared to older magnetic-disc models.

    The next hierarchical level above hard disks or other external storage devices is the RAM. We will not discuss in detail how it works
    but only take a look at some key performance metrics of the CPU at this point, which place certain performance expectations on the RAM 
    and its designers:

    The bit size of the CPU decides how many bytes of data it can access in RAM memory at the same time. 
    A 16-bit CPU can access 2 bytes (with each byte consisting of 8 bit) while a 64-bit CPU can access 8 bytes at a time.

    The processing speed of the CPU is measured in Gigahertz or Megahertz and denotes the number of operations it can perform in one second.

    From processing speed and bit size, the data rate required to keep the CPU busy can easily be computed by multiplying bit size 
    with processing speed. With modern CPUs and ever-increasing speeds, the available RAM in the market will not be fast enough 
    to match the CPU data rate requirements.

2.4 Cache Memory
    Cache Levels
    Cache memory is much faster but also significantly smaller than standard RAM. 
    It holds the data that will (or might) be used by the CPU more often. 
    In the memory hierarchy we have seen in the last section, 
    the cache plays an intermediary role between fast CPU and slow RAM and hard disk. 
    The figure 2.4.1 gives a rough overview of a typical system architecture

    The central CPU chip is connected to the outside world by a number of buses. There is a cache bus, 
    which leads to a block denoted as L2 cache, and there is a system bus as well as a memory bus that leads to the computer main memory. 
    The latter holds the comparatively large RAM while the L2 cache as well as the L1 cache are very small with the latter 
    also being a part of the CPU itself.
    The concept of L1 and L2 (and even L3) cache is further illustrated by the 2.4.2 figure, 
    which shows a multi-core CPU and its interplay with L1, L2 and L3 caches

    Level 1 cache is the fastest and smallest memory type in the cache hierarchy. In most systems, the L1 cache is not very large. 
    Mostly it is in the range of 16 to 64 kBytes, where the memory areas for instructions and data are separated from each other 
    (L1i and L1d, where "i" stands for "instruction" and "d" stands for "data". Also see "Harvard architecture" for further reference). 
    The importance of the L1 cache grows with increasing speed of the CPU. 
    In the L1 cache, the most frequently required instructions and data are buffered so that as few accesses 
    as possible to the slow main memory are required. This cache avoids delays in data transmission 
    and helps to make optimum use of the CPU's capacity.

    Level 2 cache is located close to the CPU and has a direct connection to it. 
    The information exchange between L2 cache and CPU is managed by the L2 controller on the computer main board. 
    The size of the L2 cache is usually at or below 2 megabytes. On modern multi-core processors, the L2 cache is often located 
    within the CPU itself. The choice between a processor with more clock speed or a larger L2 cache can be answered as follows: 
    With a higher clock speed, individual programs run faster, especially those with high computing requirements. 
    As soon as several programs run simultaneously, a larger cache is advantageous. Usually normal desktop computers with a processor that has a large cache are better served than with a processor that has a high clock rate.

    Level 3 cache is shared among all cores of a multicore processor. With the L3 cache, the cache coherence protocol 
    of multicore processors can work much faster. This protocol compares the caches of all cores to maintain data consistency 
    so that all processors have access to the same data at the same time. The L3 cache therefore has less the function of a cache, 
    but is intended to simplify and accelerate the cache coherence protocol and the data exchange between the cores.

    
    
    Temporal and Spatial Locality
    In algorithm design, programmers can exploit two principles to increase runtime performance:

    1.Temporal locality means that address ranges that are accessed are likely to be used again in the near future. 
    In the course of time, the same memory address is accessed relatively frequently (e.g. in a loop). 
    This property can be used at all levels of the memory hierarchy to keep memory areas accessible as quickly as possible.

    2.Spatial locality means that after an access to an address range, 
    the next access to an address in the immediate vicinity is highly probable (e.g. in arrays). 
    In the course of time, memory addresses that are very close to each other are accessed again multiple times.
    This can be exploited by moving the adjacent address areas upwards into the next hierarchy level during a memory access.



    Cache-friendly coding
    Even though we have created a two-dimensional array, it is stored in a one-dimensional succession of memory cells. 
    In our minds, the array will (probably) look like a matrix.
    In memory however, it is stored as a single line.

    As can be seen, the rows of the two-dimensional matrix are copied one after the other. 
    This format is called "row major" and is the default for both C and C++. Some other languages such as Fortran are "column major" 
    and a memory-aware programmer should always know the memory layout of the language he or she is using.

    Note that even though the row major memory layout is used in C++, this doesn't mean that all C++ libraries have the same default; 
    for example, the popular Eigen library.

    2.5 Virtual Memory
    Problems with physical memory
    Virtual memory is a very useful concept in computer architecture because it helps with making your software work 
    well given the configuration of the respective hardware on the computer it is running on.

    The idea of virtual memory stems back from a (not so long ago) time, when the random access memory (RAM) of most computers was severely 
    limited. Programers needed to treat memory as a precious resource and use it most efficiently. 
    Also, they wanted to be able to run programs even if there was not enough RAM available. At the time of writing (August 2019), 
    the amount of RAM is no longer a large concern for most computers and programs usually have enough memory available to them. 
    But in some cases, for example when trying to do video editing or when running multiple large programs at the same time, 
    the RAM memory can be exhausted. In such a case, the computer can slow down drastically.

    There are several other memory-related problems, that programmers need to know about:

    1.Holes in address space : If several programs are started one after the other and then shortly afterwards 
    some of these are terminated again, it must be ensured that the freed-up space in between the remaining programs does not remain unused. 
    If memory becomes too fragmented, it might not be possible to allocate a large block of memory due to a large-enough free contiguous block
    not being available any more.

    2.Programs writing over each other : If several programs are allowed to access the same memory address, 
    they will overwrite each others' data at this location. In some cases, this might even lead to one program reading sensitive information
    (e.g. bank account info) that was written by another program. This problem is of particular concern when writing concurrent programs 
    which run several threads at the same time.

    The basic idea of virtual memory is to separate the addresses a program may use from the addresses in physical computer memory. 
    By using a mapping function, an access to (virtual) program memory can be redirected to a real address 
    which is guaranteed to be protected from other programs.



    Expanding the available memory
    The total amount of addressable memory is limited and depends on the architecture of the system (e.g. 32-bit). 
    But what would happen if the available physical memory was below the upper bound imposed by the architecture? 
    The 2.5.1 figure illustrates the problem for such a case.

    On a typical architecture such as MIPS ("Microprocessor without interlocked pipeline stages"), 
    each program is promised to have access to an address space ranging from 0x00000000 up to 0xFFFFFFFF. 
    If however, the available physical memory is only 1GB in size, a 1-on-1 mapping would lead to undefined behavior 
    as soon as the 30-bit RAM address space were exceeded.

    With virtual memory however, a mapping is performed between the virtual address space a program sees 
    and the physical addresses of various storage devices such as the RAM but also the hard disk. 
    Mapping makes it possible for the operating system to use physical memory for the parts of a process that are currently 
    being used and back up the rest of the virtual memory to a secondary storage location such as the hard disk. 
    With virtual memory, the size of RAM is not the limit anymore as the system hard disk can be used to store information as well.

    The 2.5.2 figure illustrates the principle.

    With virtual memory, the RAM acts as a cache for the virtual memory space which resides on secondary storage devices. 
    On Windows systems, the file pagefile.sys is such a virtual memory container of varying size. 
    To speed up your system, it makes sense to adjust the system settings in a way that this file is stored on an SSD 
    instead of a slow magnetic hard drive, thus reducing the latency. On a Mac, swap files are stored in/private/var/vm/.

    In a nutshell, virtual memory guarantees us a fixed-size address space which is largely independent of the system configuration. 
    Also, the OS guarantees that the virtual address spaces of different programs do not interfere with each other.

    The task of mapping addresses and of providing each program with its own virtual address space is performed entirely 
    by the operating system, so from a programmer’s perspective, we usually don’t have to bother much about memory 
    that is being used by other processes.

    Before we take a closer look at an example though, 
    let us define two important terms which are often used in the context of caches and virtual memory:

    1.A memory page is a number of directly successive memory locations in virtual memory defined by the computer architecture 
    and by the operating system. The computer memory is divided into memory pages of equal size. 
    The use of memory pages enables the operating system to perform virtual memory management. 
    The entire working memory is divided into tiles and each address in this computer architecture is interpreted by the Memory Management Unit (MMU) 
    as a logical address and converted into a physical address.
    2.A memory frame is mostly identical to the concept of a memory page with the key difference being its location 
    in the physical main memory instead of the virtual memory.
    The 2.5.3 diagram shows two running processes and a collection of memory pages and frames.

    As can be seen, both processes have their own virtual memory space. 
    Some of the pages are mapped to frames in the physical memory and some are not. 
    If process 1 needs to use memory in the memory page that starts at address 0x1000, a page fault will occur if the required data is not there. 
    The memory page will then be mapped to a vacant memory frame in physical memory. 
    Also, note that the virtual memory addresses are not the same as the physical addresses. 
    The first memory page of process 1, which starts at the virtual address 0x0000, 
    is mapped to a memory frame that starts at the physical address 0x2000.

    In summary, virtual memory management is performed by the operating system and programmers do usually not interfere with this process. 
    The major benefit is a unique perspective on a chunk of memory for each program that is only limited in its size by the architecture 
    of the system (32 bit, 64 bit) and by the available physical memory, including the hard disk.